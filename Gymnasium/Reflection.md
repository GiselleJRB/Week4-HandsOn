I used FrozenLake Q-Learning to train an agent using the Gymnasium library. It has 16 states and 4 actions, with the agent’s task to reach the goal while avoiding holes. To simulate real-world robot navigation, the agent can slip randomly. The Q-table is initialized as a matrix of zeros with dimensions where each row represents a state and each column an action. By using the Bellman equation, the agent learns the expected reward of taking an action in a state through trial and error. By adding noise during exploration, the agent doesn’t get stuck choosing the same action early on. After training, I ran 100 test episodes and achieved a 62% success rate. This means the agent reached the goal 62 times, showing the policy learned is mostly effective despite the randomness. This is relevant to my research because it shows how agents adapt to uncertain environments through trial-based learning. In the hospital rescue scenario, the robot needs to learn to navigate unstable areas or avoid zones compromised by GPS spoofing or sensor anomalies. The FrozenLake environment uses slippery tiles, similar to how real-world anomalies make navigation unpredictable. Q-Learning helps support trust-aware decision-making, allowing agents to learn the best actions despite uncertain feedback.

